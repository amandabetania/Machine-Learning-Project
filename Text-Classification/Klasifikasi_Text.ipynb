{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdTpjThxdsKH"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('/content/data.csv')\n",
        "data.info()\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sckit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPKw-PeXTl_V",
        "outputId": "02a314d5-e056-4388-c424-0ad7945303ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement sckit-learn (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for sckit-learn\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "t1ZJ6kr-3clQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X = data[\"Ulasan IMDB\"]\n",
        "Y = data[\"Sifat Ulasan\"]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=15)\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define a function to extract semantic features\n",
        "def extract_semantic_features(text):\n",
        "    # Tokenize the input text\n",
        "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Get the BERT embeddings for the text\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "\n",
        "    # Extract features from BERT's hidden states\n",
        "    features = outputs.last_hidden_state.mean(dim=1)  # Mean pooling over tokens\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract semantic features for the training set\n",
        "X_train_semantic = [extract_semantic_features(text) for text in X_train]\n",
        "X_train_semantic = torch.cat(X_train_semantic).numpy()\n",
        "\n",
        "# Extract semantic features for the testing set\n",
        "X_test_semantic = [extract_semantic_features(text) for text in X_test]\n",
        "X_test_semantic = torch.cat(X_test_semantic).numpy()\n"
      ],
      "metadata": {
        "id": "i5DcBxhZULNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[\"Ulasan IMDB\"]\n",
        "Y = data[\"Sifat Ulasan\"]\n",
        "\n",
        "X_train_syntactic, X_test_syntactic, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=15)\n",
        "X_train_semantic, X_test_semantic, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)\n",
        "\n",
        "# Fungsi ekstraksi fitur sintaktik\n",
        "def extract_syntactic_features(text):\n",
        "    doc = nlp(text)\n",
        "    noun_count = len([token for token in doc if token.pos_ == \"NOUN\"])\n",
        "    return noun_count\n",
        "\n",
        "# Ekstraksi fitur sintaktik untuk data pelatihan\n",
        "X_train_syntactic = [extract_syntactic_features(text) for text in X_train_syntactic]\n",
        "X_train_syntactic = np.array(X_train_syntactic).reshape(-1, 1)\n",
        "\n",
        "# Ekstraksi fitur sintaktik untuk data uji\n",
        "X_test_syntactic = [extract_syntactic_features(text) for text in X_test_syntactic]\n",
        "X_test_syntactic = np.array(X_test_syntactic).reshape(-1, 1)\n",
        "\n",
        "# Ekstraksi fitur semantik untuk data pelatihan\n",
        "X_train_semantic = [extract_semantic_features(text) for text in X_train_semantic]\n",
        "X_train_semantic = np.array(X_train_semantic).reshape(-1, 1)\n",
        "\n",
        "# Ekstraksi fitur semantik untuk data uji\n",
        "X_test_semantic = [extract_semantic_features(text) for text in X_test_semantic]\n",
        "X_test_semantic = np.array(X_test_semantic).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "_J5yHmw-tnHZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "28595e7a-5412-490e-ab08-d492dbbc674c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-64fb87de7eec>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Ekstraksi fitur semantik untuk data pelatihan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mX_train_semantic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextract_semantic_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_semantic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mX_train_semantic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_semantic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-64fb87de7eec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Ekstraksi fitur semantik untuk data pelatihan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mX_train_semantic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mextract_semantic_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_semantic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mX_train_semantic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_semantic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_semantic_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat dan melatih model K-NN\n",
        "k = 3  # Jumlah tetangga terdekat\n",
        "KNN = KNeighborsClassifier(n_neighbors=k)\n",
        "KNN.fit(X_train_syntactic, Y_train)\n",
        "\n",
        "# Membuat dan melatih model SVM\n",
        "SVM = SVC(kernel='linear')\n",
        "SVM.fit(X_train_syntactic, Y_train)\n",
        "\n",
        "# Membuat dan melatih model Naive Bayes\n",
        "NB = MultinomialNB()\n",
        "NB.fit(X_train_syntactic, Y_train)\n",
        "\n",
        "# Membuat dan melatih model Random Forest\n",
        "RF = RandomForestClassifier()\n",
        "RF.fit(X_train_syntactic, Y_train)\n",
        "\n",
        "# Prediksi kategori untuk data uji\n",
        "y_pred_SVM = SVM.predict(X_test_syntactic)\n",
        "y_pred_KNN = KNN.predict(X_test_syntactic)\n",
        "Y_pred_NB = NB.predict(X_test_syntactic)\n",
        "Y_pred_RF = RF.predict(X_test_syntactic)\n",
        "\n",
        "# Menghitung akurasi\n",
        "accuracy_SVM = accuracy_score(Y_test, y_pred_SVM)\n",
        "print(\"Akurasi SVM: {:.2f}%\".format(accuracy_SVM * 100))\n",
        "\n",
        "accuracy_KNN = accuracy_score(Y_test, y_pred_KNN)\n",
        "print(\"Akurasi K-NN: {:.2f}%\".format(accuracy_KNN * 100))\n",
        "\n",
        "accuracy_NB = accuracy_score(Y_test, Y_pred_NB)\n",
        "print(\"Akurasi Naive Bayes: {:.2f}%\".format(accuracy_NB * 100))\n",
        "\n",
        "accuracy_RF = accuracy_score(Y_test, Y_pred_RF)\n",
        "print(\"Akurasi Random Forest: {:.2f}%\".format(accuracy_RF * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhfCM4k93vk5",
        "outputId": "13c51499-d954-44d5-fa8f-857f665c4380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi SVM: 50.00%\n",
            "Akurasi K-NN: 50.00%\n",
            "Akurasi Naive Bayes: 50.00%\n",
            "Akurasi Random Forest: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekstraksi fitur semantic dengan TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_semantic = tfidf_vectorizer.fit_transform(Y_train)\n",
        "X_test_semantic = tfidf_vectorizer.transform(Y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "E29su9-5FWI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat dan melatih model K-NN\n",
        "k = 4  # Jumlah tetangga terdekat\n",
        "KNN = KNeighborsClassifier(n_neighbors=k)\n",
        "KNN.fit(X_train_semantic, Y_train)\n",
        "\n",
        "# Membuat dan melatih model SVM\n",
        "SVM = SVC(kernel='linear')\n",
        "SVM.fit(X_train_semantic, Y_train)\n",
        "\n",
        "# Membuat dan melatih model Naive Bayes\n",
        "NB = MultinomialNB()\n",
        "NB.fit(X_train_semantic, Y_train)\n",
        "\n",
        "# Membuat dan melatih model Random Forest\n",
        "RF = RandomForestClassifier()\n",
        "RF.fit(X_train_semantic, Y_train)\n",
        "\n",
        "\n",
        "# Prediksi kategori untuk data uji\n",
        "y_pred_SVM = SVM.predict(X_test_semantic)\n",
        "y_pred_KNN = KNN.predict(X_test_semantic)\n",
        "Y_pred_NB = NB.predict(X_test_semantic)\n",
        "Y_pred_RF = RF.predict(X_test_semantic)\n",
        "\n",
        "# Menghitung akurasi\n",
        "accuracy_SVM = accuracy_score(Y_test, y_pred_SVM)\n",
        "print(\"Akurasi SVM: {:.2f}%\".format(accuracy_SVM * 100))\n",
        "\n",
        "accuracy_KNN = accuracy_score(Y_test, y_pred_KNN)\n",
        "print(\"Akurasi K-NN: {:.2f}%\".format(accuracy_KNN * 100))\n",
        "\n",
        "accuracy_NB = accuracy_score(Y_test, Y_pred_NB)\n",
        "print(\"Akurasi Naive Bayes: {:.2f}%\".format(accuracy_NB * 100))\n",
        "\n",
        "accuracy_RF = accuracy_score(Y_test, Y_pred_RF)\n",
        "print(\"Akurasi Random Forest: {:.2f}%\".format(accuracy_RF * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "2sGxtLpaFqgA",
        "outputId": "afc06862-28eb-4eeb-84aa-2ddd1c484788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7c573e7102c4>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Membuat dan melatih model Naive Bayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mNB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_semantic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Membuat dan melatih model Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ]
    }
  ]
}